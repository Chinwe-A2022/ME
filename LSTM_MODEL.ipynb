{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPaqfyMY7ZTHJfJIif+vgDn"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVlG_bQV7c0g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1da863c3-bdef-47e9-bff9-dcf14e32bc3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_ref = zipfile.ZipFile('/content/sensors_datasets.zip','r') #Opens the zip file\n",
        "zip_ref.extractall('/tmp') #Extracts the files into the /tmp folder\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "0aSswme478Gs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "salt_dir = \"/tmp/sensors_datasets/Salt_treaments_MLdatasets.xlsx\"\n",
        "acetic_acid_dir = \"/tmp/sensors_datasets/Acetic_acid_treatment_MLdatasets.xlsx\""
      ],
      "metadata": {
        "id": "l-oft6G_8ExF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc, mean_squared_error, mean_absolute_error\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "Vcmkh7A19BM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.compat.v1.losses.sparse_softmax_cross_entropy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "id": "E6xim9al9E7K",
        "outputId": "89d7b209-10de-45ea-b5c6-18a3c9bbce36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function tensorflow.python.ops.losses.losses_impl.sparse_softmax_cross_entropy(labels, logits, weights=1.0, scope=None, loss_collection='losses', reduction='weighted_sum_by_nonzero_weights')>"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>tensorflow.python.ops.losses.losses_impl.sparse_softmax_cross_entropy</b><br/>def sparse_softmax_cross_entropy(labels, logits, weights=1.0, scope=None, loss_collection=ops.GraphKeys.LOSSES, reduction=Reduction.SUM_BY_NONZERO_WEIGHTS)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/losses/losses_impl.py</a>Cross-entropy loss using `tf.nn.sparse_softmax_cross_entropy_with_logits`.\n",
              "\n",
              "`weights` acts as a coefficient for the loss. If a scalar is provided,\n",
              "then the loss is simply scaled by the given value. If `weights` is a\n",
              "tensor of shape `[batch_size]`, then the loss weights apply to each\n",
              "corresponding sample.\n",
              "\n",
              "Args:\n",
              "  labels: `Tensor` of shape `[d_0, d_1, ..., d_{r-1}]` (where `r` is rank of\n",
              "    `labels` and result) and dtype `int32` or `int64`. Each entry in `labels`\n",
              "    must be an index in `[0, num_classes)`. Other values will raise an\n",
              "    exception when this op is run on CPU, and return `NaN` for corresponding\n",
              "    loss and gradient rows on GPU.\n",
              "  logits: Unscaled log probabilities of shape\n",
              "    `[d_0, d_1, ..., d_{r-1}, num_classes]` and dtype `float16`, `float32` or\n",
              "    `float64`.\n",
              "  weights: Coefficients for the loss. This must be scalar or broadcastable to\n",
              "    `labels` (i.e. same rank and each dimension is either 1 or the same).\n",
              "  scope: the scope for the operations performed in computing the loss.\n",
              "  loss_collection: collection to which the loss will be added.\n",
              "  reduction: Type of reduction to apply to loss.\n",
              "\n",
              "Returns:\n",
              "  Weighted loss `Tensor` of the same type as `logits`. If `reduction` is\n",
              "  `NONE`, this has the same shape as `labels`; otherwise, it is scalar.\n",
              "\n",
              "Raises:\n",
              "  ValueError: If the shapes of `logits`, `labels`, and `weights` are\n",
              "    incompatible, or if any of them are None.\n",
              "\n",
              "@compatibility(eager)\n",
              "The `loss_collection` argument is ignored when executing eagerly. Consider\n",
              "holding on to the return value or collecting losses via a `tf.keras.Model`.\n",
              "@end_compatibility</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 1046);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "# import tensorflow_hub as hub\n",
        "from tensorflow.keras.utils import load_img\n",
        "from tensorflow.keras.utils import img_to_array\n",
        "from tensorflow import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPool2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import cv2 as cv\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "cJ8dx-0C9JTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, mean_squared_error\n",
        "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import roc_curve"
      ],
      "metadata": {
        "id": "8ZVMaj5L9MgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms, models\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.optim as optim\n",
        "from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.optim.lr_scheduler import OneCycleLR"
      ],
      "metadata": {
        "id": "LORKhpw79PbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "fhz_G_6D9TTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openpyxl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXgpcDv89Vl6",
        "outputId": "fb382dda-5967-44f9-8bbf-ecf0fe7bd90e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plant_datasets= \"/tmp/sensors_datasets\"\n",
        "salt_dir = plant_datasets + \"/Salt_treaments_MLdatasets\"\n",
        "acetic_acid_dir = plant_datasets + \"/Acetic_acid_treatment_MLdatasets\""
      ],
      "metadata": {
        "id": "DriQZEYt-Fm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List all Excel files in the folder\n",
        "excel_files = [file for file in os.listdir(plant_datasets) if file.endswith('.xlsx')]"
      ],
      "metadata": {
        "id": "HsnPLR5U_m18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a dictionary to store unique plant information\n",
        "plant_data = {}\n",
        "\n",
        "for file in excel_files:\n",
        "    # Construct the full file path\n",
        "    file_paths = os.path.join(plant_datasets, file)\n",
        "\n",
        "    # Load the Excel file\n",
        "    df = pd.read_excel(file_paths, header=None)\n",
        "\n",
        "    # Initialize an empty DataFrame to store the reshaped data\n",
        "    reshaped_df = pd.DataFrame()\n",
        "\n",
        "    # Iterate over the rows to find plant names, treatments, and sensor numbers\n",
        "    for index, row in df.iterrows():\n",
        "        # Check if the cell contains a plant name, treatment, and sensor number\n",
        "        if isinstance(row[0], str) and 'CHT' in row[0]:\n",
        "            plant_info = row[0].split(' ')\n",
        "            plant_name = plant_info[0]\n",
        "            treatment = plant_info[1]\n",
        "            sensor_number = plant_info[2]\n",
        "\n",
        "            # Create a unique key for each plant type\n",
        "            plant_key = f\"{plant_name}_{treatment}_{sensor_number}\"\n",
        "\n",
        "            # Add the extracted data to the plant_data dictionary\n",
        "            if plant_key not in plant_data:\n",
        "                plant_data[plant_key] = []\n",
        "\n",
        "            # Initialize a list to store data for the current plant key\n",
        "            plant_data_list = []\n",
        "\n",
        "            # Extract data for each hour of the day\n",
        "            for hour_index in range(1, 25):  # Assuming 24 hours of data\n",
        "                hour_data = df.iloc[index + hour_index].tolist()\n",
        "                hour = hour_data[0]  # Assuming the first cell is the hour\n",
        "                parameters = hour_data[1:]  # Assuming the rest of the cells are the parameters\n",
        "                plant_data[plant_key].append({'hour': hour, 'parameters': parameters})\n",
        "\n",
        "    # Print the plant data\n",
        "    for plant_key, data in plant_data.items():\n",
        "        print(f\"Plant: {plant_key}\")\n",
        "        for hour_data in data:\n",
        "            print(f\"Hour: {hour_data['hour']}, Parameters: {hour_data['parameters']}\")\n",
        "        print(\"\\n\")"
      ],
      "metadata": {
        "id": "kjU9D_KF9cHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate variation and categorize it for each set of four parameters\n",
        "def categorize_variation(hour_data):\n",
        "    variations = {}\n",
        "    categories = {}\n",
        "\n",
        "    # Iterate over each hour in the data\n",
        "    for hour_data in data:\n",
        "        hour = hour_data['hour']\n",
        "        parameters = hour_data['parameters']\n",
        "        hour_variations = []\n",
        "        hour_categories = []\n",
        "\n",
        "        for i in range(0, len(parameters), 4):\n",
        "            param_set = [parameters[i + 1], parameters[i + 3]]\n",
        "\n",
        "            # Check if all values in the set are numeric\n",
        "            if all(isinstance(x, (int, float)) for x in param_set):\n",
        "                variation = pd.Series(param_set).std() / pd.Series(param_set).mean()\n",
        "                hour_variations.append(variation)\n",
        "\n",
        "                # Categorize the variation\n",
        "                if variation <= 1.13:\n",
        "                    category = 1  # good\n",
        "                elif variation <= 1.17:\n",
        "                    category = 2  # medium\n",
        "                else:\n",
        "                    category = 3  # bad\n",
        "                hour_categories.append(category)\n",
        "            else:\n",
        "                # Skip the calculation for non-numeric parameter sets\n",
        "                hour_variations.append(None)\n",
        "                hour_categories.append(None)\n",
        "\n",
        "        variations[hour] = hour_variations\n",
        "        categories[hour] = hour_categories\n",
        "\n",
        "    return variations, categories\n",
        "\n",
        "for plant_key, data in plant_data.items():\n",
        "    print(f\"Plant: {plant_key}\")\n",
        "    variations, categories = categorize_variation(data)\n",
        "    for hour, hour_variations in variations.items():\n",
        "        print(f\"Hour: {hour}, Variations: {hour_variations}, Categories: {categories[hour]}\")\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "xEVij9XU_3uQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to normalize data\n",
        "def normalize_data(df, scaler):\n",
        "    df[['temperature', 'moisture', 'soil_fertility', 'light_intensity']] = scaler.transform(df[['temperature', 'moisture', 'soil_fertility', 'light_intensity']])\n",
        "    return df"
      ],
      "metadata": {
        "id": "gRDKHjBuABBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to create sequences for LSTM\n",
        "def create_sequences(df, n_steps=1):\n",
        "    X, y = [], []\n",
        "    for i in range(len(df) - n_steps):\n",
        "        X.append(df[['temperature', 'moisture', 'soil_fertility', 'light_intensity']].iloc[i:(i + n_steps)].values)\n",
        "        y.append(df['category'].iloc[i + n_steps])\n",
        "    return np.array(X), np.array(y)"
      ],
      "metadata": {
        "id": "OQ0DjILyAQdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "df_salt = categorize_variation(salt_dir)\n",
        "df_acetic_acid = categorize_variation(acetic_acid_dir)"
      ],
      "metadata": {
        "id": "zUrK--dUARv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df_salt, temp_df_salt = train_test_split(df_salt, test_size=0.3, random_state=42)\n",
        "\n",
        "# Check if temp_df_salt has enough samples for further splitting\n",
        "if len(temp_df_salt) > 1:\n",
        "    val_df_salt, test_df_salt = train_test_split(temp_df_salt, test_size=0.33, random_state=42)\n",
        "else:\n",
        "    # Handle the case where there are not enough samples for further splitting\n",
        "    print(\"Not enough samples in temp_df_salt for further splitting. Consider adjusting test_size or using a different approach.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EOm0kdxCFoV",
        "outputId": "3ebb5a42-0ded-4ae7-c81e-b77392f1c592"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not enough samples in temp_df_salt for further splitting. Consider adjusting test_size or using a different approach.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Adjust the test_size to ensure temp_df_salt has at least 3 samples\n",
        "train_df_salt, temp_df_salt = train_test_split(df_salt, test_size=0.4, random_state=42)\n",
        "\n",
        "# Check the number of samples in temp_df_salt\n",
        "if len(temp_df_salt) >= 3:\n",
        "    val_df_salt, test_df_salt = train_test_split(temp_df_salt, test_size=0.5, random_state=42)\n",
        "else:\n",
        "    print(\"Not enough samples in temp_df_salt for further splitting.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yKzCxbkCYz3",
        "outputId": "b0a96039-70b1-4e7d-d27f-7eddc64eca43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not enough samples in temp_df_salt for further splitting.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Size of df_salt:\", len(df_salt))"
      ],
      "metadata": {
        "id": "xi1_rBcYChMM",
        "outputId": "f029736a-1e22-46d8-b517-6db6d74aae4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of df_salt: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data\n",
        "train_df_salt, temp_df_salt = train_test_split(df_salt, test_size=0.3, random_state=42)\n",
        "val_df_salt, test_df_salt = train_test_split(temp_df_salt, test_size=1/3, random_state=42)\n",
        "\n",
        "train_df_acetic_acid, temp_df_acetic_acid = train_test_split(df_acetic_acid, test_size=0.3, random_state=42)\n",
        "val_df_acetic_acid, test_df_acetic_acid = train_test_split(temp_df_acetic_acid, test_size=1/3, random_state=42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "wPCwiNaMAx6l",
        "outputId": "154b8c13-8279-4949-8c22-055aeaa88930"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "With n_samples=1, test_size=0.3333333333333333 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-61fe49a72174>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Split the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrain_df_salt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_df_salt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_salt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mval_df_salt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df_salt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_df_salt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_df_acetic_acid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_df_acetic_acid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_acetic_acid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2561\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2562\u001b[0;31m     n_train, n_test = _validate_shuffle_split(\n\u001b[0m\u001b[1;32m   2563\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_test_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2564\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2235\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_train\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2236\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   2237\u001b[0m             \u001b[0;34m\"With n_samples={}, test_size={} and train_size={}, the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2238\u001b[0m             \u001b[0;34m\"resulting train set will be empty. Adjust any of the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: With n_samples=1, test_size=0.3333333333333333 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
          ]
        }
      ]
    }
  ]
}